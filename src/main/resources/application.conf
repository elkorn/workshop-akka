akka {
  loglevel = DEBUG
  log-config-on-start = off

  actor {
    debug {
      receive = on
      autoreceive = on
      lifecycle = on
      event-stream = on
    }

    deployment {
      /kitchen-roundrobin {
        router = round-robin-pool
        nr-of-instances = 3
      }

      /kitchen-random {
        router = random-pool
        nr-of-instances = 3
      }

      # All routees share the same mailbox. Actors that are or become idle are sent new messages.
      /kitchen-balancing {
        router = balancing-pool
        nr-of-instances = 3
      }

      # Routee with the smallest mailbox queue will get new messages.
      # This can have a drawback - the actor with the smallest mailbox queue may be very slow and just happened to not
      # have very many requests to service. This can lead to snowballing, effectively increasing latency instead of
      # reducing it.
      /kitchen-smallest-mailbox {
        router = smallest-mailbox-pool
        nr-of-instances = 3
      }

      /kitchen-broadcast {
        router = smallest-mailbox-pool
        nr-of-instances = 3
      }

      # Using this pool will broadcast messages to all of the routees and take the first response, discarding the rest.
      /kitchen-scatter-gather {
        router = scatter-gather-pool
        nr-of-instances = 3
        within = 10 seconds # Defines that a reponse must arrive within this timeframe, otherwise a failure message is returned.
      }

      # Similar to scatter-gather. Sends a message to a randomly picked routee and after a small delay picks another one
      # to send to. This helps to avoid latency, since there is a probability that any actor may be bogged down by some
      # operation.
      # In distributed data stores, redundant operations can dramatically drop tail latency at the expense of increased
      # system load. Dynamo and Cassandra are prime examples of systems that take advantage of this technique.
      /kitchen-tail-chopping {
        router = tail-chopping-pool
        nr-of-instances = 3
      }

      # Allows to balance messages fairly across a cluster, resilient in cases when the size of the cluster changes dynamically.
      # The messages can implement akka.routing.ConsistentHashingRouter.ConsistentHashable, which gives them a hashkey or
      # they can be wrapped in a akka.routing.ConsistentHashingRouter.ConsistentHashableEnvelope.
      # When a routee is added, it takes its share of elements from the other routees and when it is removed,
      # its elements are shared between the remaining routees.
      # The most common use case for this pool is for scaling the actor system across multiple nodes using akka-cluster.
      /kitchen-consistent-hashing {
        router = consistent-hashing-pool
        nr-of-instances = 3
      }
    }
  }
}

McBurger {
  operational-delay {
    value= "1 s"
    variance = "500 ms"
  }
}